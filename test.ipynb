{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from align_utils import (\n",
    "    cls_align_beats,\n",
    "    il_align_beats,\n",
    "    select_acc_dtype,\n",
    "    select_device,\n",
    "    evaluate_accuracy_acil,\n",
    "    init_w_fe\n",
    ")\n",
    "from data_utils import ESC50SplitDataset, pad_collate\n",
    "from downloader import download_esc50\n",
    "from model_utils import (\n",
    "    BEATsWithHead,\n",
    "    expand_classifier,\n",
    "    load_beats_backbone,\n",
    "    load_beats_model,\n",
    "    maybe_resume_checkpoint,\n",
    ")\n",
    "from split_esc50 import ESC_ROOT, make_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2cef20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESC-50 already exists at Dataset/ESC-50-master\n",
      "Loaded 50 targets\n",
      "Saved splits to /Users/yyy/Desktop/research/BEATS-acil/esc50_25_5x5_splits.json\n"
     ]
    }
   ],
   "source": [
    "download_esc50(Path(\"Dataset\"))\n",
    "esc_root = ESC_ROOT\n",
    "if not esc_root.exists():\n",
    "    raise FileNotFoundError(f\"ESC-50 root not found: {esc_root}\")\n",
    "splits = make_splits(\n",
    "    esc_root=esc_root,\n",
    "    test_fold=1,\n",
    "    seed=2026,\n",
    ")\n",
    "out_path = Path(\"esc50_25_5x5_splits.json\")\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "print(f\"Saved splits to {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0569bbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyy/Desktop/research/BEATS-acil/.venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEATs model loaded from: checkpoints/BEATs_iter3_plus_AS2M.pt\n",
      "BEATsWithHead(\n",
      "  (beats): BEATs(\n",
      "    (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    (patch_embedding): Conv2d(1, 512, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (dropout_input): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): TransformerEncoder(\n",
      "      (pos_conv): Sequential(\n",
      "        (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "        (1): SamePad()\n",
      "        (2): GELU(approximate='none')\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "            (relative_attention_bias): Embedding(320, 12)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1-11): 11 x TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
      "            (relative_attention_bias): Embedding(320, 12)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=25, bias=False)\n",
      ")\n",
      "Training mode: True, device: mps\n"
     ]
    }
   ],
   "source": [
    "beats_checkpoint_path = Path(\"checkpoints/BEATs_iter3_plus_AS2M.pt\")\n",
    "if beats_checkpoint_path.exists():\n",
    "            device = select_device(\"mps\")\n",
    "            beats = load_beats_model(beats_checkpoint_path, device).to(device)\n",
    "            print(f\"BEATs model loaded from: {beats_checkpoint_path}\")\n",
    "            train_dataset = ESC50SplitDataset(\n",
    "                splits=splits,\n",
    "                audio_dir=esc_root / \"audio\",\n",
    "                use_split=\"train\",\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=4,\n",
    "                shuffle=True,\n",
    "                num_workers=0,\n",
    "                collate_fn=pad_collate,\n",
    "            )\n",
    "            val_dataset = ESC50SplitDataset(\n",
    "                splits=splits,\n",
    "                audio_dir=esc_root / \"audio\",\n",
    "                use_split=\"test\",\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=4,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                collate_fn=pad_collate,\n",
    "            )\n",
    "            #将.wav变成了fbank用于训练，加上了分类头[num_classes 25],beats自带的函数\n",
    "            model = BEATsWithHead(beats, num_classes=25).to(device)\n",
    "            print(model)\n",
    "            model.train()\n",
    "            print(f\"Training mode: {model.training}, device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31f9f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['cfg', 'model'])\n",
      "num_classes: None\n"
     ]
    }
   ],
   "source": [
    "origckpt = torch.load(Path(\"checkpoints/BEATs_iter3_plus_AS2M.pt\"), map_location=\"cpu\")\n",
    "print(type(origckpt))\n",
    "print(origckpt.keys())\n",
    "print(\"num_classes:\", origckpt.get(\"num_classes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e51cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_extract_proj.weight (768, 512) torch.float32\n",
      "post_extract_proj.bias (768,) torch.float32\n",
      "patch_embedding.weight (512, 1, 16, 16) torch.float32\n",
      "encoder.pos_conv.0.bias (768,) torch.float32\n",
      "encoder.pos_conv.0.weight_g (1, 1, 128) torch.float32\n",
      "encoder.pos_conv.0.weight_v (768, 48, 128) torch.float32\n",
      "encoder.layers.0.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.0.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.0.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.0.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.0.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.0.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.0.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.0.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.0.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.0.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.0.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.0.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.0.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.0.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.0.fc2.bias (768,) torch.float32\n",
      "encoder.layers.0.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.0.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.1.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.1.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.1.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.1.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.1.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.1.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.1.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.1.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.1.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.1.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.1.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.1.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.1.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.1.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.1.fc2.bias (768,) torch.float32\n",
      "encoder.layers.1.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.1.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.2.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.2.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.2.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.2.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.2.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.2.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.2.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.2.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.2.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.2.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.2.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.2.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.2.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.2.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.2.fc2.bias (768,) torch.float32\n",
      "encoder.layers.2.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.2.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.3.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.3.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.3.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.3.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.3.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.3.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.3.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.3.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.3.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.3.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.3.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.3.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.3.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.3.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.3.fc2.bias (768,) torch.float32\n",
      "encoder.layers.3.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.3.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.4.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.4.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.4.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.4.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.4.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.4.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.4.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.4.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.4.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.4.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.4.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.4.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.4.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.4.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.4.fc2.bias (768,) torch.float32\n",
      "encoder.layers.4.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.4.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.5.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.5.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.5.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.5.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.5.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.5.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.5.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.5.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.5.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.5.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.5.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.5.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.5.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.5.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.5.fc2.bias (768,) torch.float32\n",
      "encoder.layers.5.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.5.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.6.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.6.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.6.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.6.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.6.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.6.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.6.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.6.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.6.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.6.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.6.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.6.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.6.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.6.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.6.fc2.bias (768,) torch.float32\n",
      "encoder.layers.6.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.6.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.7.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.7.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.7.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.7.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.7.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.7.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.7.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.7.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.7.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.7.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.7.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.7.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.7.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.7.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.7.fc2.bias (768,) torch.float32\n",
      "encoder.layers.7.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.7.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.8.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.8.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.8.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.8.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.8.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.8.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.8.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.8.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.8.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.8.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.8.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.8.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.8.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.8.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.8.fc2.bias (768,) torch.float32\n",
      "encoder.layers.8.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.8.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.9.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.9.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.9.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.9.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.9.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.9.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.9.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.9.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.9.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.9.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.9.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.9.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.9.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.9.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.9.fc2.bias (768,) torch.float32\n",
      "encoder.layers.9.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.9.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.10.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.10.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.10.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.10.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.10.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.10.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.10.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.10.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.10.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.10.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.10.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.10.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.10.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.10.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.10.fc2.bias (768,) torch.float32\n",
      "encoder.layers.10.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.10.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.11.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.11.self_attn.k_proj.bias (768,) torch.float32\n",
      "encoder.layers.11.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.11.self_attn.v_proj.bias (768,) torch.float32\n",
      "encoder.layers.11.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.11.self_attn.q_proj.bias (768,) torch.float32\n",
      "encoder.layers.11.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "encoder.layers.11.self_attn.out_proj.bias (768,) torch.float32\n",
      "encoder.layers.11.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "encoder.layers.11.self_attn_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.11.self_attn_layer_norm.bias (768,) torch.float32\n",
      "encoder.layers.11.fc1.weight (3072, 768) torch.float32\n",
      "encoder.layers.11.fc1.bias (3072,) torch.float32\n",
      "encoder.layers.11.fc2.weight (768, 3072) torch.float32\n",
      "encoder.layers.11.fc2.bias (768,) torch.float32\n",
      "encoder.layers.11.final_layer_norm.weight (768,) torch.float32\n",
      "encoder.layers.11.final_layer_norm.bias (768,) torch.float32\n",
      "encoder.layer_norm.weight (768,) torch.float32\n",
      "encoder.layer_norm.bias (768,) torch.float32\n",
      "layer_norm.weight (512,) torch.float32\n",
      "layer_norm.bias (512,) torch.float32\n",
      "encoder.layers.0.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.0.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.0.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.1.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.1.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.1.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.2.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.2.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.2.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.3.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.3.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.3.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.4.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.4.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.4.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.5.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.5.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.5.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.6.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.6.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.6.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.7.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.7.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.7.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.8.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.8.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.8.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.9.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.9.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.9.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.10.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.10.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.10.self_attn.grep_linear.bias (8,) torch.float32\n",
      "encoder.layers.11.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "encoder.layers.11.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "encoder.layers.11.self_attn.grep_linear.bias (8,) torch.float32\n"
     ]
    }
   ],
   "source": [
    "state = origckpt[\"model\"]\n",
    "for i, (k, v) in enumerate(state.items()):\n",
    "    print(k, tuple(v.shape), v.dtype)\n",
    "    #原BEATs的参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa0f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": model.beats.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "    ]\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338dd20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyy/Desktop/research/BEATS-acil/.venv/lib/python3.11/site-packages/torchaudio/compliance/kaldi.py:616: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [498, 257]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:38.)\n",
      "  spectrum = torch.fft.rfft(strided_input).abs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Eval mode: False (should be False)\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([11, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([ 6,  6, 18, 22], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16,  8, 24], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 21], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 2, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 22, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "Epoch 1/5 - loss: 1.5045 - val_acc: 0.9300 - time: 71.1s\n",
      "Training mode: True (should be True)\n",
      "200\n",
      "Eval mode: False (should be False)\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([11, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([ 6,  6, 11,  6], device='mps:0'), tensor([ 6,  6, 18, 22], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([23, 23,  8, 24], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 21], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "Epoch 2/5 - loss: 0.1373 - val_acc: 0.9650 - time: 70.7s\n",
      "Training mode: True (should be True)\n",
      "200\n",
      "Eval mode: False (should be False)\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([11, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([ 6,  6, 18, 22], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([16, 23,  8, 24], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 2, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "Epoch 3/5 - loss: 0.0582 - val_acc: 0.9650 - time: 71.2s\n",
      "Training mode: True (should be True)\n",
      "200\n",
      "Eval mode: False (should be False)\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([11, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([ 6,  6, 11,  6], device='mps:0'), tensor([ 6,  6, 18, 22], device='mps:0'), tensor([23, 23, 23, 16], device='mps:0'), tensor([16, 23,  8, 24], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "Epoch 4/5 - loss: 0.0345 - val_acc: 0.9600 - time: 70.2s\n",
      "Training mode: True (should be True)\n",
      "200\n",
      "Eval mode: False (should be False)\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([11, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([ 6,  6, 11,  6], device='mps:0'), tensor([ 6,  6, 18, 22], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16,  8, 24], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "[tensor([11, 11, 11, 11], device='mps:0'), tensor([11, 11, 11, 11], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([8, 8, 8, 8], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([24, 24, 24, 24], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([4, 4, 4, 4], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([16, 16, 16, 16], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([14, 14, 14, 14], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([19, 19, 19, 19], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([13, 13, 13, 13], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([0, 0, 0, 0], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([6, 6, 6, 6], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([23, 23, 23, 23], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([9, 9, 9, 9], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([10, 10, 10, 10], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([20, 20, 20, 20], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([2, 2, 2, 2], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([22, 22, 22, 22], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([21, 21, 21, 21], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([1, 1, 1, 1], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([3, 3, 3, 3], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([17, 17, 17, 17], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([15, 15, 15, 15], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([12, 12, 12, 12], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([18, 18, 18, 18], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([7, 7, 7, 7], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0'), tensor([5, 5, 5, 5], device='mps:0')]\n",
      "Epoch 5/5 - loss: 0.0365 - val_acc: 0.9400 - time: 69.3s\n",
      "Training mode: True (should be True)\n",
      "Saved fine-tuned checkpoint to: checkpoints/beats_finetuned_base25.pt\n",
      "Saved BEATs backbone checkpoint to: checkpoints/beats_base25_backbone.pt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5 + 1):\n",
    "    epoch_start = time.perf_counter()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for audio, padding_mask, targets in train_loader:\n",
    "        audio = audio.to(device)\n",
    "        padding_mask = padding_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()#梯度清零\n",
    "        logits = model(audio, padding_mask)\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    print(num_batches) #200\n",
    "\n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"Eval mode: {model.training} (should be False)\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for audio, padding_mask, targets in val_loader:\n",
    "            audio = audio.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(audio, padding_mask)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "        print(all_preds)\n",
    "        print(all_targets)\n",
    "    acc = correct / max(total, 1)\n",
    "    epoch_time = time.perf_counter() - epoch_start\n",
    "    print(\n",
    "        f\"Epoch {epoch}/5 - loss: {avg_loss:.4f} - val_acc: {acc:.4f} \"\n",
    "        f\"- time: {epoch_time:.1f}s\"\n",
    "    )\n",
    "    model.train()\n",
    "    print(f\"Training mode: {model.training} (should be True)\")\n",
    "\n",
    "ckpt_out = Path(\"checkpoints/beats_finetuned_base25.pt\")\n",
    "ckpt_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"cfg\": model.beats.cfg.__dict__,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"num_classes\": 25,\n",
    "    },\n",
    "    ckpt_out,\n",
    ")\n",
    "print(f\"Saved fine-tuned checkpoint to: {ckpt_out}\")\n",
    "\n",
    "backbone_out = Path(\"checkpoints/beats_base25_backbone.pt\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"cfg\": model.beats.cfg.__dict__,\n",
    "        \"model\": model.beats.state_dict(),\n",
    "    },\n",
    "    backbone_out,\n",
    ")\n",
    "print(f\"Saved BEATs backbone checkpoint to: {backbone_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bf55fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['cfg', 'model', 'num_classes'])\n",
      "num_classes: 25\n",
      "beats.post_extract_proj.weight (768, 512) torch.float32\n",
      "beats.post_extract_proj.bias (768,) torch.float32\n",
      "beats.patch_embedding.weight (512, 1, 16, 16) torch.float32\n",
      "beats.encoder.pos_conv.0.bias (768,) torch.float32\n",
      "beats.encoder.pos_conv.0.weight_g (1, 1, 128) torch.float32\n",
      "beats.encoder.pos_conv.0.weight_v (768, 48, 128) torch.float32\n",
      "beats.encoder.layers.0.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.0.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.0.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.0.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.0.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.0.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.0.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.0.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.0.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.0.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.0.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.0.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.0.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.0.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.0.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.0.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.0.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.0.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.0.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.0.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.1.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.1.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.1.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.1.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.1.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.1.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.1.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.1.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.1.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.1.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.1.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.1.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.1.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.1.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.2.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.2.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.2.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.2.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.2.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.2.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.2.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.2.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.2.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.2.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.2.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.2.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.2.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.2.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.3.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.3.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.3.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.3.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.3.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.3.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.3.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.3.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.3.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.3.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.3.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.3.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.3.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.3.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.4.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.4.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.4.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.4.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.4.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.4.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.4.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.4.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.4.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.4.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.4.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.4.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.4.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.4.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.5.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.5.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.5.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.5.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.5.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.5.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.5.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.5.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.5.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.5.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.5.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.5.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.5.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.5.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.6.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.6.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.6.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.6.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.6.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.6.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.6.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.6.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.6.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.6.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.6.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.6.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.6.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.6.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.7.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.7.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.7.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.7.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.7.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.7.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.7.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.7.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.7.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.7.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.7.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.7.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.7.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.7.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.8.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.8.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.8.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.8.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.8.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.8.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.8.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.8.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.8.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.8.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.8.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.8.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.8.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.8.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.9.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.9.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.9.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.9.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.9.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.9.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.9.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.9.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.9.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.9.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.9.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.9.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.9.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.9.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.10.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.10.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.10.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.10.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.10.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.10.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.10.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.10.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.10.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.10.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.10.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.10.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.10.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.10.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.self_attn.grep_a (1, 12, 1, 1) torch.float32\n",
      "beats.encoder.layers.11.self_attn.k_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.11.self_attn.k_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.self_attn.v_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.11.self_attn.v_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.self_attn.q_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.11.self_attn.q_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.self_attn.out_proj.weight (768, 768) torch.float32\n",
      "beats.encoder.layers.11.self_attn.out_proj.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.self_attn.grep_linear.weight (8, 64) torch.float32\n",
      "beats.encoder.layers.11.self_attn.grep_linear.bias (8,) torch.float32\n",
      "beats.encoder.layers.11.self_attn.relative_attention_bias.weight (320, 12) torch.float32\n",
      "beats.encoder.layers.11.self_attn_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.11.self_attn_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.fc1.weight (3072, 768) torch.float32\n",
      "beats.encoder.layers.11.fc1.bias (3072,) torch.float32\n",
      "beats.encoder.layers.11.fc2.weight (768, 3072) torch.float32\n",
      "beats.encoder.layers.11.fc2.bias (768,) torch.float32\n",
      "beats.encoder.layers.11.final_layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layers.11.final_layer_norm.bias (768,) torch.float32\n",
      "beats.encoder.layer_norm.weight (768,) torch.float32\n",
      "beats.encoder.layer_norm.bias (768,) torch.float32\n",
      "beats.layer_norm.weight (512,) torch.float32\n",
      "beats.layer_norm.bias (512,) torch.float32\n",
      "classifier.weight (25, 768) torch.float32\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(Path(\"checkpoints/beats_finetuned_base25.pt\"), map_location=\"mps\")\n",
    "print(type(ckpt))\n",
    "print(ckpt.keys())\n",
    "print(\"num_classes:\", ckpt.get(\"num_classes\"))\n",
    "state = ckpt[\"model\"]\n",
    "for i, (k, v) in enumerate(state.items()):\n",
    "    print(k, tuple(v.shape), v.dtype)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5830a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 768])\n",
      "tensor([[ 0.0321,  0.0500, -0.0302,  0.0999,  0.0265,  0.0260, -0.0355,  0.0527],\n",
      "        [-0.0303,  0.0814, -0.0121,  0.0439, -0.0198,  0.0510,  0.0005,  0.1116]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "w = ckpt[\"model\"][\"classifier.weight\"]  # 如果key不存在会报错\n",
    "print(w.shape)\n",
    "print(w[:2, :8])  # 只打印一小块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11098d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEATs backbone loaded from: checkpoints/beats_base25_backbone.pt\n",
      "Seen classes after phase 1: 30\n",
      "Seen classes after phase 2: 35\n",
      "Seen classes after phase 3: 40\n",
      "Seen classes after phase 4: 45\n",
      "Seen classes after phase 5: 50\n",
      "Alignment mode: False, device: mps\n",
      "num_classes: 25\n",
      "classifier.weight(before): torch.Size([25, 768])\n",
      "R shape: torch.Size([4000, 4000])\n",
      "classifier.weight(after): torch.Size([25, 4000])\n",
      "Base phase acc: 0.9650\n"
     ]
    }
   ],
   "source": [
    "backbone_ckpt_path = Path(\"checkpoints/beats_base25_backbone.pt\")\n",
    "if backbone_ckpt_path.exists():\n",
    "    device = select_device(\"mps\")\n",
    "    beats = load_beats_backbone(backbone_ckpt_path, device)\n",
    "    beats.to(device)\n",
    "    beats.eval()\n",
    "    for p in beats.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"BEATs backbone loaded from: {backbone_ckpt_path}\")\n",
    "    base_classes = list(splits[\"base_classes\"])[:25]\n",
    "    incremental_groups = splits[\"incremental_classes\"][:5]\n",
    "    if incremental_groups:\n",
    "        for idx, group in enumerate(incremental_groups, start=1):\n",
    "            count = len(base_classes) + sum(\n",
    "                len(g) for g in incremental_groups[:idx]\n",
    "            )\n",
    "            print(f\"Seen classes after phase {idx}: {count}\")\n",
    "    model = BEATsWithHead(beats, num_classes=len(base_classes)).to(device)\n",
    "    model.eval()\n",
    "    print(f\"Alignment mode: {model.training}, device: {device}\")\n",
    "    maybe_resume_checkpoint(model, \"\", device)\n",
    "\n",
    "    base_class_to_idx = {cid: idx for idx, cid in enumerate(base_classes)}\n",
    "\n",
    "    base_train_dataset = ESC50SplitDataset(\n",
    "        splits=splits,\n",
    "        audio_dir=esc_root / \"audio\",\n",
    "        use_split=\"train\",\n",
    "        class_ids=base_classes,\n",
    "        class_to_idx=base_class_to_idx,\n",
    "    )\n",
    "    base_train_loader = DataLoader(\n",
    "        base_train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=pad_collate,\n",
    "    )\n",
    "    base_val_dataset = ESC50SplitDataset(\n",
    "        splits=splits,\n",
    "        audio_dir=esc_root / \"audio\",\n",
    "        use_split=\"test\",\n",
    "        class_ids=base_classes,\n",
    "        class_to_idx=base_class_to_idx,\n",
    "    )\n",
    "    base_val_loader = DataLoader(\n",
    "        base_val_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=pad_collate,\n",
    "    )\n",
    "acc_cil: list[float] = []\n",
    "forget_rate: list[float] = []\n",
    "print(\"num_classes:\", len(base_classes))\n",
    "print(\"classifier.weight(before):\", model.classifier.weight.shape)\n",
    "#增量学习前评估beats的准确度并初始化R\n",
    "W_fe = init_w_fe(768, 4000, device, dtype=select_acc_dtype(device)) \n",
    "R, W = cls_align_beats(\n",
    "    train_loader=base_train_loader,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    num_classes=len(base_classes),\n",
    "    rg=1e-3,\n",
    "    W_fe = W_fe\n",
    ")\n",
    "print(\"R shape:\", R.shape)\n",
    "print(\"classifier.weight(after):\", model.classifier.weight.shape)\n",
    "base_acc = evaluate_accuracy_acil(base_val_loader, model, device, W, W_fe)\n",
    "acc_cil.append(base_acc)\n",
    "print(f\"Base phase acc: {base_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b9e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 classes: [21, 33, 43, 3, 45]\n",
      "new_num_classes: 30\n",
      "classifier.weight(Phase 1): torch.Size([30, 4000])\n",
      "W shape after expand: torch.Size([4000, 30])\n",
      "seen_classes (phase 1, n=30):\n",
      "torch.Size([4000, 30])\n",
      "torch.Size([4000, 4000])\n",
      "val classes: 30\n",
      "Phase 1/5 - acc: 0.9583 - base_now: 0.9550 - forget: 0.0100\n",
      "Phase 2 classes: [9, 49, 5, 0, 15]\n",
      "new_num_classes: 35\n",
      "classifier.weight(Phase 2): torch.Size([35, 4000])\n",
      "W shape after expand: torch.Size([4000, 35])\n",
      "seen_classes (phase 2, n=35):\n",
      "torch.Size([4000, 35])\n",
      "torch.Size([4000, 4000])\n",
      "val classes: 35\n",
      "Phase 2/5 - acc: 0.9643 - base_now: 0.9550 - forget: 0.0100\n",
      "Phase 3 classes: [28, 31, 40, 36, 26]\n",
      "new_num_classes: 40\n",
      "classifier.weight(Phase 3): torch.Size([40, 4000])\n",
      "W shape after expand: torch.Size([4000, 40])\n",
      "seen_classes (phase 3, n=40):\n",
      "torch.Size([4000, 40])\n",
      "torch.Size([4000, 4000])\n",
      "val classes: 40\n",
      "Phase 3/5 - acc: 0.9625 - base_now: 0.9550 - forget: 0.0100\n",
      "Phase 4 classes: [35, 39, 38, 14, 6]\n",
      "new_num_classes: 45\n",
      "classifier.weight(Phase 4): torch.Size([45, 4000])\n",
      "W shape after expand: torch.Size([4000, 45])\n",
      "seen_classes (phase 4, n=45):\n",
      "torch.Size([4000, 45])\n",
      "torch.Size([4000, 4000])\n",
      "val classes: 45\n",
      "Phase 4/5 - acc: 0.9556 - base_now: 0.9550 - forget: 0.0100\n",
      "Phase 5 classes: [41, 47, 32, 20, 7]\n",
      "new_num_classes: 50\n",
      "classifier.weight(Phase 5): torch.Size([50, 4000])\n",
      "W shape after expand: torch.Size([4000, 50])\n",
      "seen_classes (phase 5, n=50):\n",
      "torch.Size([4000, 50])\n",
      "torch.Size([4000, 4000])\n",
      "val classes: 50\n",
      "Phase 5/5 - acc: 0.9550 - base_now: 0.9550 - forget: 0.0100\n",
      "Average accuracy: 0.9601\n",
      "Saved incremental checkpoint to: checkpoints/beats_base25_incremental.pt\n"
     ]
    }
   ],
   "source": [
    "for phase_idx, inc_classes in enumerate(incremental_groups, start=1):\n",
    "    print(f\"Phase {phase_idx} classes: {inc_classes}\")\n",
    "    new_num_classes = len(base_classes) + sum(\n",
    "        len(g) for g in incremental_groups[:phase_idx]\n",
    "    )\n",
    "    print(\"new_num_classes:\", new_num_classes)\n",
    "    expand_classifier(model, new_num_classes)\n",
    "    print(f\"classifier.weight(Phase {phase_idx}):\", model.classifier.weight.shape)\n",
    "\n",
    "    # 同步扩展 W: [fe_dim, old_C] -> [fe_dim, new_C]\n",
    "    if W.size(1) < new_num_classes:\n",
    "        W_new = torch.zeros(\n",
    "            W.size(0), new_num_classes,\n",
    "            device=W.device, dtype=W.dtype\n",
    "        )\n",
    "        W_new[:, :W.size(1)] = W\n",
    "        W = W_new\n",
    "\n",
    "    print(\"W shape after expand:\", W.shape)  # 例如 [2000, 30]\n",
    "\n",
    "    seen_classes = list(base_classes)\n",
    "    for g in incremental_groups[:phase_idx]:\n",
    "        seen_classes.extend(g)\n",
    "    phase_class_to_idx = {cid: idx for idx, cid in enumerate(seen_classes)}\n",
    "    print(f\"seen_classes (phase {phase_idx}, n={len(seen_classes)}):\")\n",
    "\n",
    "    inc_train_dataset = ESC50SplitDataset(\n",
    "        splits=splits,\n",
    "        audio_dir=esc_root / \"audio\",\n",
    "        use_split=\"train\",\n",
    "        class_ids=inc_classes,\n",
    "        class_to_idx=phase_class_to_idx,\n",
    "    )\n",
    "    inc_train_loader = DataLoader(\n",
    "        inc_train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=pad_collate,\n",
    "    )\n",
    "\n",
    "    if isinstance(R, tuple):\n",
    "        if len(R) == 2:\n",
    "            R, W = R\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected R tuple length: {len(R)}\")\n",
    "\n",
    "    R, W = il_align_beats(\n",
    "        train_loader=inc_train_loader,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        num_classes=new_num_classes,\n",
    "        R=R,\n",
    "        W=W,\n",
    "        repeat=1,\n",
    "        W_fe=W_fe\n",
    "    )\n",
    "\n",
    "\n",
    "    val_dataset = ESC50SplitDataset(\n",
    "        splits=splits,\n",
    "        audio_dir=esc_root / \"audio\",\n",
    "        use_split=\"test\",\n",
    "        class_ids=seen_classes,\n",
    "        class_to_idx=phase_class_to_idx,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=pad_collate,\n",
    "    )\n",
    "\n",
    "    print(\"val classes:\", len(val_dataset.class_to_idx))\n",
    "    acc = evaluate_accuracy_acil(val_loader, model, device, W, W_fe)\n",
    "    acc_cil.append(acc)\n",
    "\n",
    "    #验证增量后beats阶段的25类数据的准确度\n",
    "    base_acc_now = evaluate_accuracy_acil(base_val_loader, model, device, W, W_fe)\n",
    "    forget = acc_cil[0] - base_acc_now\n",
    "    forget_rate.append(forget)\n",
    "\n",
    "    print(\n",
    "        f\"Phase {phase_idx}/{len(incremental_groups)} \"\n",
    "        f\"- acc: {acc:.4f} \"\n",
    "        f\"- base_now: {base_acc_now:.4f} \"\n",
    "        f\"- forget: {forget:.4f}\"\n",
    "    )\n",
    "\n",
    "if acc_cil:\n",
    "    avg = sum(acc_cil) / len(acc_cil)\n",
    "    print(f\"Average accuracy: {avg:.4f}\")\n",
    "else:\n",
    "    print(\"Average accuracy: n/a (no phases evaluated)\")\n",
    "\n",
    "ckpt_out = Path(\"checkpoints/beats_base25_incremental.pt\")\n",
    "ckpt_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"cfg\": model.beats.cfg.__dict__,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"num_classes\": model.classifier.out_features,\n",
    "    },\n",
    "    ckpt_out,\n",
    ")\n",
    "print(f\"Saved incremental checkpoint to: {ckpt_out}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
